{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.1+cu117\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('torch version:',torch.__version__)\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_multi_car_racing import bezier\n",
    "from gymnasium.utils import seeding\n",
    "\n",
    "np_random, seed = seeding.np_random(7)\n",
    "\n",
    "def get_track(control_points=None, mindst=0.08, np_random=np_random):\n",
    "    if control_points is not None:\n",
    "        a = np.array(control_points)\n",
    "        x, y, _ = bezier.get_bezier_curve(a=a, rad=0.2, edgy=0.2, numpoints=40)\n",
    "    else:\n",
    "        a = bezier.get_random_points(n=12, scale=PLAYFIELD, mindst=mindst, np_random=np_random) // 30 * 30\n",
    "        x, y, _ = bezier.get_bezier_curve(a=a, rad=0.2, edgy=0.2, numpoints=40)\n",
    "\n",
    "    x, y = np.clip(0, x.max(), x), np.clip(0, y.max(), y)\n",
    "\n",
    "    return a, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_track(track, checkpoints=None):\n",
    "    # Plot track\n",
    "    plt.plot(track[:,1], track[:,0], linewidth=9, color=[0.4, 0.4, 0.4])\n",
    "\n",
    "    # Plot checkpoints\n",
    "    if checkpoints is not None:\n",
    "        for i in range(len(checkpoints)):\n",
    "            y, x = checkpoints[i,:]\n",
    "            plt.plot(x, y, 'o', markersize=2, color=\"tab:orange\")\n",
    "            plt.text(x, y, str(i), fontsize=10, color=\"black\")\n",
    "    \n",
    "    plt.xlim(-20,360)\n",
    "    plt.ylim(-20,360)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor(np.array([102, 230, 102])/255.)\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(coordinates, img_size=(32,32)):\n",
    "    max_value = 330 if coordinates.max() < 330 else coordinates.max()\n",
    "\n",
    "    if coordinates[:,0].min() < 0:\n",
    "        coordinates[:,0] = coordinates[:,0] - coordinates[:,0].min()\n",
    "    if coordinates[:,1].min() < 0:\n",
    "        coordinates[:,1] = coordinates[:,1] - coordinates[:,1].min()\n",
    "    coordinates = coordinates // (max_value/(img_size[0]-1))\n",
    "    coordinates = coordinates.astype(np.int32)\n",
    "\n",
    "    # Clip to 0-31\n",
    "    coordinates = np.clip(coordinates, 0, img_size[0]-1)\n",
    "    \n",
    "    img = np.zeros(img_size)\n",
    "\n",
    "    for i in range(coordinates.shape[0]):\n",
    "        img[coordinates[i,1], coordinates[i,0]] = 1\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new color map\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "grass_green = np.array([102/255, 230/255, 102/255, 1])\n",
    "road_gray = np.array([0.4, 0.4, 0.4, 1])\n",
    "newcolors = np.array([grass_green, road_gray])\n",
    "new_cmp = ListedColormap(newcolors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = np.load('X_30k.npy') * 30\n",
    "Y = np.load('complexities_30k.npy')\n",
    "Y = Y/0.25*10\n",
    "Y = np.clip(Y, 0, 9)\n",
    "Y = Y.round(0)\n",
    "\n",
    "class_num = 10\n",
    "\n",
    "input_size = 32 # Input size\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Model\n",
    "z_size = 100\n",
    "generator_layer_size = [256, 512, 1024]\n",
    "discriminator_layer_size = [1024, 512, 256]\n",
    "\n",
    "# Training\n",
    "epochs = 30  # Train epochs\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 30000/30000\r"
     ]
    }
   ],
   "source": [
    "# Get images track images\n",
    "\n",
    "IMG_SIZE = (32,32)\n",
    "\n",
    "X_img = []  # Track images\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    # Generate random track\n",
    "    checkpoints, x, y = get_track(control_points=X[i], np_random=np_random)\n",
    "    # Get image from track\n",
    "    track = np.array([x, y]).T\n",
    "    X_img.append(get_image(track, img_size=IMG_SIZE))\n",
    "\n",
    "    print(f\"Progress: {i+1}/{X.shape[0]}\", end=\"\\r\")\n",
    "\n",
    "X_img = np.array(X_img).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class TracksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, tracks, difficulties):\n",
    "        self.tracks = tracks\n",
    "        self.difficulties = difficulties\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tracks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = TF.to_tensor(self.tracks[index])\n",
    "        y = torch.tensor(self.difficulties[index])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_img, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TracksDataset(X_train, Y_train)\n",
    "val_dataset = TracksDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, generator_layer_size, z_size, input_size, class_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.z_size = z_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.label_emb = nn.Embedding(class_num, class_num)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.z_size + class_num, generator_layer_size[0]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(generator_layer_size[0], generator_layer_size[1]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(generator_layer_size[1], generator_layer_size[2]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(generator_layer_size[2], self.input_size * self.input_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        \n",
    "        # Reshape z\n",
    "        z = z.view(-1, self.z_size)\n",
    "        \n",
    "        # One-hot vector to embedding vector\n",
    "        c = self.label_emb(labels)\n",
    "        \n",
    "        # Concat image & label\n",
    "        x = torch.cat([z, c], 1)\n",
    "        \n",
    "        # Generator out\n",
    "        out = self.model(x)\n",
    "        \n",
    "        return out.view(-1, self.input_size, self.input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, discriminator_layer_size, input_size, class_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(class_num, class_num)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_size * self.input_size + class_num, discriminator_layer_size[0]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(discriminator_layer_size[0], discriminator_layer_size[1]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(discriminator_layer_size[1], discriminator_layer_size[2]),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(discriminator_layer_size[2], 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        \n",
    "        # Reshape fake image\n",
    "        x = x.view(-1, self.input_size * self.input_size)\n",
    "        \n",
    "        # One-hot vector to embedding vector\n",
    "        c = self.label_emb(labels)\n",
    "        \n",
    "        # Concat image & label\n",
    "        x = torch.cat([x, c], 1)\n",
    "        \n",
    "        # Discriminator out\n",
    "        out = self.model(x)\n",
    "        \n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator\n",
    "generator = Generator(generator_layer_size, z_size, input_size, class_num).to(device)\n",
    "# Define discriminator\n",
    "discriminator = Discriminator(discriminator_layer_size, input_size, class_num).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "    \n",
    "    # Init gradient\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    # Building z\n",
    "    z = Variable(torch.randn(batch_size, z_size)).to(device)\n",
    "    \n",
    "    # Building fake labels\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, class_num, batch_size))).to(device)\n",
    "    \n",
    "    # Generating fake images\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    \n",
    "    # Disciminating fake images\n",
    "    validity = discriminator(fake_images, fake_labels)\n",
    "    \n",
    "    # Calculating discrimination loss (fake images)\n",
    "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).to(device))\n",
    "    \n",
    "    # Backword propagation\n",
    "    g_loss.backward()\n",
    "    \n",
    "    #  Optimizing generator\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return g_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
    "    \n",
    "    # Init gradient \n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # Disciminating real images\n",
    "    real_validity = discriminator(real_images, labels)\n",
    "    \n",
    "    # Calculating discrimination loss (real images)\n",
    "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).to(device))\n",
    "    \n",
    "    # Building z\n",
    "    z = Variable(torch.randn(batch_size, z_size)).to(device)\n",
    "    \n",
    "    # Building fake labels\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, class_num, batch_size))).to(device)\n",
    "    \n",
    "    # Generating fake images\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    \n",
    "    # Disciminating fake images\n",
    "    fake_validity = discriminator(fake_images, fake_labels)\n",
    "    \n",
    "    # Calculating discrimination loss (fake images)\n",
    "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).to(device))\n",
    "    \n",
    "    # Sum two losses\n",
    "    d_loss = real_loss + fake_loss\n",
    "    \n",
    "    # Backword propagation\n",
    "    d_loss.backward()\n",
    "    \n",
    "    # Optimizing discriminator\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return d_loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n",
      "g_loss: 2.166813611984253, d_loss: 0.553512454032898\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('Starting epoch {}...'.format(epoch+1))\n",
    "    \n",
    "    for i, (points, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Train data\n",
    "        real_points = Variable(points).to(device).long()\n",
    "        labels = Variable(labels).to(device).long()\n",
    "        \n",
    "        # Set generator train\n",
    "        generator.train()\n",
    "        \n",
    "        # Train discriminator\n",
    "        d_loss = discriminator_train_step(len(real_points), discriminator,\n",
    "                                          generator, d_optimizer, criterion,\n",
    "                                          real_points, labels)\n",
    "        \n",
    "        # Train generator\n",
    "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
    "    \n",
    "    # Set generator eval\n",
    "    generator.eval()\n",
    "    \n",
    "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n",
    "    \n",
    "    # Building z \n",
    "    z = Variable(torch.randn(class_num-1, z_size)).to(device)\n",
    "    \n",
    "    # Labels 0 ~ 8\n",
    "    labels = Variable(torch.LongTensor(np.arange(class_num-1))).to(device)\n",
    "    \n",
    "    # Generating points\n",
    "    sample_points = generator(z, labels).unsqueeze(1).data.cpu()\n",
    "\n",
    "    break\n",
    "\n",
    "    grid = make_grid(sample_points, nrow=3, normalize=True).permute(1,2,0).numpy()\n",
    "    plt.imshow(grid)\n",
    "    plt.show()\n",
    "\n",
    "    # plt.figure(figsize=(4,4))\n",
    "    # # Show points\n",
    "    # for i in range(9):\n",
    "    #     plt.subplot(3,3,i+1)\n",
    "    #     points = sample_points[i][0].cpu().numpy().reshape(12,2)\n",
    "    #     a, x, y = get_track(points*300)\n",
    "    #     track = np.array([x, y]).T\n",
    "    #     track_img = get_image(track, img_size=(64,64))\n",
    "    #     plt.imshow(track_img, cmap=new_cmp)\n",
    "    #     plt.axis('off')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum_learning_ws-qGcE4JgX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
