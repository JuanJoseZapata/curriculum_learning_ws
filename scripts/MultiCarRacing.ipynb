{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/home/developer/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    }
   ],
   "source": [
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_multi_car_racing import multi_car_racing_pyglet as multi_car_racing\n",
    "\n",
    "n_agents = 2\n",
    "# Step 1: Load the PettingZoo environment\n",
    "env = multi_car_racing.env(n_agents=n_agents, direction=\"CCW\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:674: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Wrap the environment for Tianshou interfacing\n",
    "env = PettingZooEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import MultiAgentPolicyManager, RandomPolicy\n",
    "\n",
    "# Step 3: Define policies for each agent\n",
    "policies = MultiAgentPolicyManager([RandomPolicy() for _ in range(n_agents)], env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert the env to vector format\n",
    "env = DummyVectorEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:674: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n"
     ]
    }
   ],
   "source": [
    "from tianshou.data import Collector\n",
    "\n",
    "# Step 5: Construct the Collector, which interfaces the policies with the vectorised environment\n",
    "collector = Collector(policies, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Execute the environment with the agents playing for 1 episode, and render a frame every 0.1 seconds\n",
    "result = collector.collect(n_episode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.common import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gym.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            state_shape=observation_space.shape or observation_space.n,\n",
    "            action_shape=env.action_space.shape or env.action_space.n,\n",
    "            hidden_sizes=[32, 32, 32, 32],\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=0.9,\n",
    "            estimation_step=3,\n",
    "            target_update_freq=320,\n",
    "        )\n",
    "\n",
    "    if agent_opponent is None:\n",
    "        agent_opponent = RandomPolicy()\n",
    "\n",
    "    agents = [agent_opponent, agent_learn]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    env = multi_car_racing.env(n_agents=n_agents, direction=\"CCW\", render_mode=\"human\")\n",
    "    return PettingZooEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:674: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======== Step 1: Environment setup =========\n",
    "train_envs = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "test_envs = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "\n",
    "# seed\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "train_envs.seed(seed)\n",
    "test_envs.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Step 2: Agent setup =========\n",
    "policy, optim, agents = _get_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:674: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 0,\n",
       " 'n/st': 640,\n",
       " 'rews': array([], dtype=float64),\n",
       " 'lens': array([], dtype=int64),\n",
       " 'idxs': array([], dtype=int64),\n",
       " 'rew': 0,\n",
       " 'len': 0,\n",
       " 'rew_std': 0,\n",
       " 'len_std': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======== Step 3: Collector setup =========\n",
    "train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(20_000, len(train_envs)),\n",
    "    exploration_noise=True,\n",
    ")\n",
    "test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "# policy.set_eps(1)\n",
    "train_collector.collect(n_step=64 * 10)  # batch size * training_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Step 4: Callback functions setup =========\n",
    "def save_best_fn(policy):\n",
    "    model_save_path = os.path.join(\"log\", \"rps\", \"dqn\", \"policy.pth\")\n",
    "    os.makedirs(os.path.join(\"log\", \"rps\", \"dqn\"), exist_ok=True)\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= 0.6\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.1)\n",
    "\n",
    "def test_fn(epoch, env_step):\n",
    "    policy.policies[agents[1]].set_eps(0.05)\n",
    "\n",
    "def reward_metric(rews):\n",
    "    return rews[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   5%|5         | 50/1000 [00:03<00:46, 20.28it/s, car_1/loss=19.726, env_step=50, len=0, n/ep=0, n/st=50, rew=0.00]/home/developer/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:674: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n",
      "Epoch #1: 1001it [00:56, 17.61it/s, car_1/loss=3.003, env_step=1000, len=0, n/ep=0, n/st=50, rew=0.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -58.249344 ± 20.330170, best_reward: -58.249344 ± 20.330170 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:54, 18.50it/s, car_1/loss=5.600, env_step=2000, len=0, n/ep=0, n/st=50, rew=0.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -100.060877 ± 0.032160, best_reward: -58.249344 ± 20.330170 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1001it [00:55, 18.13it/s, car_1/loss=2.389, env_step=3000, len=2984, n/ep=0, n/st=50, rew=-100.05]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -57.044845 ± 33.788629, best_reward: -57.044845 ± 33.788629 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1001it [00:55, 17.91it/s, car_1/loss=2.615, env_step=4000, len=1154, n/ep=0, n/st=50, rew=-120.62]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -57.412519 ± 26.506414, best_reward: -57.044845 ± 33.788629 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1001it [00:57, 17.41it/s, car_1/loss=9.016, env_step=5000, len=1056, n/ep=0, n/st=50, rew=-79.57]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -61.369216 ± 29.536643, best_reward: -57.044845 ± 33.788629 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 1001it [00:56, 17.67it/s, car_1/loss=11.134, env_step=6000, len=798, n/ep=0, n/st=50, rew=-72.91]                          \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# ======== Step 5: Run the trainer =========\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[39m=\u001b[39m offpolicy_trainer(\n\u001b[1;32m      3\u001b[0m     policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[1;32m      4\u001b[0m     train_collector\u001b[39m=\u001b[39;49mtrain_collector,\n\u001b[1;32m      5\u001b[0m     test_collector\u001b[39m=\u001b[39;49mtest_collector,\n\u001b[1;32m      6\u001b[0m     max_epoch\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     step_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     step_per_collect\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     episode_per_test\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     train_fn\u001b[39m=\u001b[39;49mtrain_fn,\n\u001b[1;32m     12\u001b[0m     test_fn\u001b[39m=\u001b[39;49mtest_fn,\n\u001b[1;32m     13\u001b[0m     stop_fn\u001b[39m=\u001b[39;49mstop_fn,\n\u001b[1;32m     14\u001b[0m     save_best_fn\u001b[39m=\u001b[39;49msave_best_fn,\n\u001b[1;32m     15\u001b[0m     update_per_step\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m     test_in_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     17\u001b[0m     reward_metric\u001b[39m=\u001b[39;49mreward_metric,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39m# return result, policy.policies[agents[1]]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m==========Result==========\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/trainer/offpolicy.py:133\u001b[0m, in \u001b[0;36moffpolicy_trainer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moffpolicy_trainer\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Wrapper for OffPolicyTrainer run method.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[39m    It is identical to ``OffpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[39m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m OffpolicyTrainer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/trainer/base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[1;32m    443\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[1;32m    444\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/trainer/base.py:315\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     test_stat, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_step()\n\u001b[1;32m    316\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run:\n\u001b[1;32m    317\u001b[0m         epoch_stat\u001b[39m.\u001b[39mupdate(test_stat)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/trainer/base.py:344\u001b[0m, in \u001b[0;36mBaseTrainer.test_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    343\u001b[0m stop_fn_flag \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m test_result \u001b[39m=\u001b[39m test_episode(\n\u001b[1;32m    345\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_collector, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch,\n\u001b[1;32m    346\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisode_per_test, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_metric\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m rew, rew_std \u001b[39m=\u001b[39m test_result[\u001b[39m\"\u001b[39m\u001b[39mrew\u001b[39m\u001b[39m\"\u001b[39m], test_result[\u001b[39m\"\u001b[39m\u001b[39mrew_std\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_epoch \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward \u001b[39m<\u001b[39m rew:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/trainer/utils.py:27\u001b[0m, in \u001b[0;36mtest_episode\u001b[0;34m(policy, collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m test_fn:\n\u001b[1;32m     26\u001b[0m     test_fn(epoch, global_step)\n\u001b[0;32m---> 27\u001b[0m result \u001b[39m=\u001b[39m collector\u001b[39m.\u001b[39;49mcollect(n_episode\u001b[39m=\u001b[39;49mn_episode)\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m reward_metric:\n\u001b[1;32m     29\u001b[0m     rew \u001b[39m=\u001b[39m reward_metric(result[\u001b[39m\"\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/data/collector.py:295\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m action_remap \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mmap_action(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mact)\n\u001b[1;32m    294\u001b[0m \u001b[39m# step in env\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m obs_next, rew, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    296\u001b[0m     action_remap,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    297\u001b[0m     ready_env_ids\n\u001b[1;32m    298\u001b[0m )\n\u001b[1;32m    299\u001b[0m done \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogical_or(terminated, truncated)\n\u001b[1;32m    301\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    302\u001b[0m     obs_next\u001b[39m=\u001b[39mobs_next,\n\u001b[1;32m    303\u001b[0m     rew\u001b[39m=\u001b[39mrew,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     info\u001b[39m=\u001b[39minfo\n\u001b[1;32m    308\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/env/venvs.py:345\u001b[0m, in \u001b[0;36mBaseVectorEnv.step\u001b[0;34m(self, action, id)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(action) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mid\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mid\u001b[39m):\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers[j]\u001b[39m.\u001b[39;49msend(action[i])\n\u001b[1;32m    346\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mid\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/env/worker/dummy.py:38\u001b[0m, in \u001b[0;36mDummyEnvWorker.send\u001b[0;34m(self, action, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/tianshou/env/pettingzoo_env.py:96\u001b[0m, in \u001b[0;36mPettingZooEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Dict, List[\u001b[39mint\u001b[39m], \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, Dict]:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     98\u001b[0m     observation, rew, term, trunc, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlast()\n\u001b[1;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maction_mask\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m observation:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:81\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:116\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39magent_selection\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py:29\u001b[0m, in \u001b[0;36mAssertOutOfBoundsWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m         action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[39mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m         action\n\u001b[1;32m     28\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39maction is not in action space\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:116\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39magent_selection\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:81\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:116\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39magent_selection\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/conversions.py:325\u001b[0m, in \u001b[0;36mparallel_to_aec_wrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actions[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection] \u001b[39m=\u001b[39m action\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agent_selector\u001b[39m.\u001b[39mis_last():\n\u001b[0;32m--> 325\u001b[0m     obss, rews, terminations, truncations, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_actions)\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_observations \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(obss)\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterminations \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(terminations)\n",
      "File \u001b[0;32m~/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:484\u001b[0m, in \u001b[0;36mparallel_env.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworld\u001b[39m.\u001b[39mStep(\u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mFPS, \u001b[39m6\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m    482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mFPS\n\u001b[0;32m--> 484\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender(\u001b[39m\"\u001b[39;49m\u001b[39mstate_pixels\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    486\u001b[0m step_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_agents)\n\u001b[1;32m    487\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:590\u001b[0m, in \u001b[0;36mparallel_env.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[1;32m    589\u001b[0m \u001b[39mfor\u001b[39;00m cur_car_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_agents):\n\u001b[0;32m--> 590\u001b[0m     result\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_window(cur_car_id, mode))\n\u001b[1;32m    592\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mstack(result, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:664\u001b[0m, in \u001b[0;36mparallel_env._render_window\u001b[0;34m(self, car_id, mode)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_road()\n\u001b[1;32m    663\u001b[0m \u001b[39mfor\u001b[39;00m geom \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer[car_id]\u001b[39m.\u001b[39monetime_geoms:\n\u001b[0;32m--> 664\u001b[0m    geom\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    665\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer[car_id]\u001b[39m.\u001b[39monetime_geoms \u001b[39m=\u001b[39m []\n\u001b[1;32m    666\u001b[0m t\u001b[39m.\u001b[39mdisable()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py:177\u001b[0m, in \u001b[0;36mGeom.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrs):\n\u001b[1;32m    176\u001b[0m     attr\u001b[39m.\u001b[39menable()\n\u001b[0;32m--> 177\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender1()\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrs:\n\u001b[1;32m    179\u001b[0m     attr\u001b[39m.\u001b[39mdisable()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py:250\u001b[0m, in \u001b[0;36mFilledPolygon.render1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39melse\u001b[39;00m: glBegin(GL_TRIANGLES)\n\u001b[1;32m    249\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv:\n\u001b[0;32m--> 250\u001b[0m     glVertex3f(p[\u001b[39m0\u001b[39;49m], p[\u001b[39m1\u001b[39m],\u001b[39m0\u001b[39m)  \u001b[39m# draw each vertex\u001b[39;00m\n\u001b[1;32m    251\u001b[0m glEnd()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======== Step 5: Run the trainer =========\n",
    "result = offpolicy_trainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=50,\n",
    "    step_per_epoch=1000,\n",
    "    step_per_collect=50,\n",
    "    episode_per_test=10,\n",
    "    batch_size=64,\n",
    "    train_fn=train_fn,\n",
    "    test_fn=test_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    update_per_step=0.1,\n",
    "    test_in_train=False,\n",
    "    reward_metric=reward_metric,\n",
    ")\n",
    "\n",
    "# return result, policy.policies[agents[1]]\n",
    "print(f\"\\n==========Result==========\\n{result}\")\n",
    "print(\"\\n(the trained policy can be accessed via policy.policies[agents[1]])\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test parallel environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API test\n",
      "Track generation: 1076..1354 -> 278-tiles track\n",
      "Track generation: 1093..1370 -> 277-tiles track\n",
      "Track generation: 1237..1552 -> 315-tiles track\n",
      "Track generation: 943..1183 -> 240-tiles track\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpettingzoo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest\u001b[39;00m \u001b[39mimport\u001b[39;00m api_test\n\u001b[1;32m      3\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     api_test(env, num_cycles\u001b[39m=\u001b[39;49m\u001b[39m1_000_000\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/test/api_test.py:506\u001b[0m, in \u001b[0;36mapi_test\u001b[0;34m(env, num_cycles, verbose_progress)\u001b[0m\n\u001b[1;32m    502\u001b[0m test_observation_action_spaces(env, agent_0)\n\u001b[1;32m    504\u001b[0m progress_report(\u001b[39m\"\u001b[39m\u001b[39mFinished test_observation_action_spaces\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 506\u001b[0m play_test(env, observation_0, num_cycles)\n\u001b[1;32m    508\u001b[0m progress_report(\u001b[39m\"\u001b[39m\u001b[39mFinished play test\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    510\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(env\u001b[39m.\u001b[39mrewards, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mrewards must be a dict\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/test/api_test.py:356\u001b[0m, in \u001b[0;36mplay_test\u001b[0;34m(env, observation_0, num_cycles)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    352\u001b[0m     accumulated_rewards[agent] \u001b[39m==\u001b[39m reward\n\u001b[1;32m    353\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mreward returned by last is not the accumulated rewards in its rewards dict\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m accumulated_rewards[agent] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 356\u001b[0m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    358\u001b[0m \u001b[39mfor\u001b[39;00m a, rew \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    359\u001b[0m     accumulated_rewards[a] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rew\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:81\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:116\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39magent_selection\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py:29\u001b[0m, in \u001b[0;36mAssertOutOfBoundsWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m         action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[39mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m         action\n\u001b[1;32m     28\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39maction is not in action space\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:116\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39magent_selection\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:81\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/wrappers/base.py:116\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActionType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39magent_selection\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pettingzoo/utils/conversions.py:325\u001b[0m, in \u001b[0;36mparallel_to_aec_wrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actions[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection] \u001b[39m=\u001b[39m action\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agent_selector\u001b[39m.\u001b[39mis_last():\n\u001b[0;32m--> 325\u001b[0m     obss, rews, terminations, truncations, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_actions)\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_observations \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(obss)\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterminations \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(terminations)\n",
      "File \u001b[0;32m~/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:484\u001b[0m, in \u001b[0;36mparallel_env.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworld\u001b[39m.\u001b[39mStep(\u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mFPS, \u001b[39m6\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m    482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mFPS\n\u001b[0;32m--> 484\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender(\u001b[39m\"\u001b[39;49m\u001b[39mstate_pixels\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    486\u001b[0m step_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_agents)\n\u001b[1;32m    487\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:590\u001b[0m, in \u001b[0;36mparallel_env.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[1;32m    589\u001b[0m \u001b[39mfor\u001b[39;00m cur_car_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_agents):\n\u001b[0;32m--> 590\u001b[0m     result\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_window(cur_car_id, mode))\n\u001b[1;32m    592\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mstack(result, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:662\u001b[0m, in \u001b[0;36mparallel_env._render_window\u001b[0;34m(self, car_id, mode)\u001b[0m\n\u001b[1;32m    660\u001b[0m gl\u001b[39m.\u001b[39mglViewport(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, VP_W, VP_H)\n\u001b[1;32m    661\u001b[0m t\u001b[39m.\u001b[39menable()\n\u001b[0;32m--> 662\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_road()\n\u001b[1;32m    663\u001b[0m \u001b[39mfor\u001b[39;00m geom \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer[car_id]\u001b[39m.\u001b[39monetime_geoms:\n\u001b[1;32m    664\u001b[0m    geom\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/workspace/multi_car_racing/gym_multi_car_racing/multi_car_racing_pyglet.py:698\u001b[0m, in \u001b[0;36mparallel_env.render_road\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[1;32m    697\u001b[0m     \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m--> 698\u001b[0m         gl\u001b[39m.\u001b[39;49mglVertex3f(k\u001b[39m*\u001b[39mx \u001b[39m+\u001b[39m k, k\u001b[39m*\u001b[39my \u001b[39m+\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    699\u001b[0m         gl\u001b[39m.\u001b[39mglVertex3f(k\u001b[39m*\u001b[39mx \u001b[39m+\u001b[39m \u001b[39m0\u001b[39m, k\u001b[39m*\u001b[39my \u001b[39m+\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    700\u001b[0m         gl\u001b[39m.\u001b[39mglVertex3f(k\u001b[39m*\u001b[39mx \u001b[39m+\u001b[39m \u001b[39m0\u001b[39m, k\u001b[39m*\u001b[39my \u001b[39m+\u001b[39m k, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/pyglet/__init__.py:328\u001b[0m, in \u001b[0;36m_ModuleProxy.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_module, name)\n\u001b[1;32m    330\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pettingzoo.test import api_test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_test(env, num_cycles=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "pettingzoo_env_to_vec_env takes in a pettingzoo ParallelEnv. Can create a parallel_env with pistonball.parallel_env() or convert it from an AEC env with `from pettingzoo.utils.conversions import aec_to_parallel; aec_to_parallel(env)``",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msupersuit\u001b[39;00m \u001b[39mimport\u001b[39;00m pettingzoo_env_to_vec_env_v1, frame_stack_v1, concat_vec_envs_v1\n\u001b[1;32m      3\u001b[0m \u001b[39m#env = frame_stack_v1(env, 4)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m env \u001b[39m=\u001b[39m pettingzoo_env_to_vec_env_v1(env)\n\u001b[1;32m      5\u001b[0m env \u001b[39m=\u001b[39m concat_vec_envs_v1(env, num_vec_envs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, num_cpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, base_class\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstable_baselines3\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/supersuit/vector/vector_constructors.py:83\u001b[0m, in \u001b[0;36mpettingzoo_env_to_vec_env_v1\u001b[0;34m(parallel_env)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpettingzoo_env_to_vec_env_v1\u001b[39m(parallel_env):\n\u001b[0;32m---> 83\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m     84\u001b[0m         parallel_env, ParallelEnv\n\u001b[1;32m     85\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mpettingzoo_env_to_vec_env takes in a pettingzoo ParallelEnv. Can create a parallel_env with pistonball.parallel_env() or convert it from an AEC env with `from pettingzoo.utils.conversions import aec_to_parallel; aec_to_parallel(env)``\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mhasattr\u001b[39m(\n\u001b[1;32m     87\u001b[0m         parallel_env, \u001b[39m\"\u001b[39m\u001b[39mpossible_agents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39menvironment passed to pettingzoo_env_to_vec_env must have possible_agents attribute.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m MarkovVectorEnv(parallel_env)\n",
      "\u001b[0;31mAssertionError\u001b[0m: pettingzoo_env_to_vec_env takes in a pettingzoo ParallelEnv. Can create a parallel_env with pistonball.parallel_env() or convert it from an AEC env with `from pettingzoo.utils.conversions import aec_to_parallel; aec_to_parallel(env)``"
     ]
    }
   ],
   "source": [
    "from supersuit import pettingzoo_env_to_vec_env_v1, frame_stack_v1, concat_vec_envs_v1\n",
    "\n",
    "#env = frame_stack_v1(env, 4)\n",
    "env = pettingzoo_env_to_vec_env_v1(env)\n",
    "env = concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=1, base_class='stable_baselines3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv_checker\u001b[39;00m \u001b[39mimport\u001b[39;00m check_env\n\u001b[0;32m----> 2\u001b[0m check_env(env)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/env_checker.py:390\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_env\u001b[39m(env: gym\u001b[39m.\u001b[39mEnv, warn: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, skip_render_check: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39m    Check that an environment follows Gym API.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[39m    This is particularly useful when using a custom environment.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39m        True by default (useful for the CI)\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    391\u001b[0m         env, gym\u001b[39m.\u001b[39mEnv\n\u001b[1;32m    392\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mYour environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m     \u001b[39m# ============= Check the spaces (observation and action) ================\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     _check_spaces(env)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m log_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLogs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39m# Define model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m# Single input\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, tensorboard_log\u001b[39m=\u001b[39;49mlog_path, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# MultiInputPolicy since we now have multiple inputs (image and speed)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#model = PPO(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=log_path, gamma=0.98)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:104\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m ):\n\u001b[0;32m--> 104\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    105\u001b[0m         policy,\n\u001b[1;32m    106\u001b[0m         env,\n\u001b[1;32m    107\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    108\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m    109\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m    110\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[1;32m    111\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[1;32m    112\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[1;32m    113\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[1;32m    114\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m    115\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m    116\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[1;32m    117\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m    118\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m    119\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    120\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    121\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    122\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    123\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    124\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[1;32m    125\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[1;32m    126\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[1;32m    127\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[1;32m    128\u001b[0m         ),\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     \u001b[39m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:81\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     60\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     82\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[1;32m     83\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m     84\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     85\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m     86\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     87\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     88\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m     89\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m     90\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     91\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     92\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[1;32m     93\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m     94\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:169\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m env \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     env \u001b[39m=\u001b[39m maybe_make_env(env, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 169\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_env(env, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, monitor_wrapper)\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:246\u001b[0m, in \u001b[0;36mBaseAlgorithm._wrap_env\u001b[0;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    245\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWrapping the env in a VecTransposeImage.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 246\u001b[0m         env \u001b[39m=\u001b[39m VecTransposeImage(env)\n\u001b[1;32m    248\u001b[0m \u001b[39mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:42\u001b[0m, in \u001b[0;36mVecTransposeImage.__init__\u001b[0;34m(self, venv, skip)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     observation_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_space(venv\u001b[39m.\u001b[39mobservation_space)\n\u001b[0;32m---> 42\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(venv, observation_space\u001b[39m=\u001b[39;49mobservation_space)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:336\u001b[0m, in \u001b[0;36mVecEnvWrapper.__init__\u001b[0;34m(self, venv, observation_space, action_space)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    330\u001b[0m     venv: VecEnv,\n\u001b[1;32m    331\u001b[0m     observation_space: Optional[spaces\u001b[39m.\u001b[39mSpace] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m     action_space: Optional[spaces\u001b[39m.\u001b[39mSpace] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m ):\n\u001b[1;32m    334\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvenv \u001b[39m=\u001b[39m venv\n\u001b[0;32m--> 336\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    337\u001b[0m         num_envs\u001b[39m=\u001b[39;49mvenv\u001b[39m.\u001b[39;49mnum_envs,\n\u001b[1;32m    338\u001b[0m         observation_space\u001b[39m=\u001b[39;49mobservation_space \u001b[39mor\u001b[39;49;00m venv\u001b[39m.\u001b[39;49mobservation_space,\n\u001b[1;32m    339\u001b[0m         action_space\u001b[39m=\u001b[39;49maction_space \u001b[39mor\u001b[39;49;00m venv\u001b[39m.\u001b[39;49maction_space,\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_attributes \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(inspect\u001b[39m.\u001b[39mgetmembers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:77\u001b[0m, in \u001b[0;36mVecEnv.__init__\u001b[0;34m(self, num_envs, observation_space, action_space)\u001b[0m\n\u001b[1;32m     74\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `render_mode` attribute is not defined in your environment. It will be set to None.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m     render_modes \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_envs)]\n\u001b[0;32m---> 77\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m     78\u001b[0m     render_mode \u001b[39m==\u001b[39m render_modes[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m render_mode \u001b[39min\u001b[39;00m render_modes\n\u001b[1;32m     79\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mrender_mode mode should be the same for all environments\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m=\u001b[39m render_modes[\u001b[39m0\u001b[39m]\n\u001b[1;32m     82\u001b[0m render_modes \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "# Define model\n",
    "\n",
    "# Single input\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path, gamma=0.99)\n",
    "\n",
    "# MultiInputPolicy since we now have multiple inputs (image and speed)\n",
    "#model = PPO(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=log_path, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1110..1392 -> 282-tiles track\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,96,96,3) into shape (96,96,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m5_000\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:246\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    236\u001b[0m     \u001b[39mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[1;32m    237\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[1;32m    244\u001b[0m     iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 246\u001b[0m     total_timesteps, callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    247\u001b[0m         total_timesteps,\n\u001b[1;32m    248\u001b[0m         callback,\n\u001b[1;32m    249\u001b[0m         reset_num_timesteps,\n\u001b[1;32m    250\u001b[0m         tb_log_name,\n\u001b[1;32m    251\u001b[0m         progress_bar,\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    256\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:424\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset()  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m# pytype: enable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:110\u001b[0m, in \u001b[0;36mVecTransposeImage.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[np\u001b[39m.\u001b[39mndarray, Dict]:\n\u001b[1;32m    107\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m    Reset all environments\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_observations(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mreset())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m     76\u001b[0m     obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_seeds[env_idx])\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_obs(env_idx, obs)\n\u001b[1;32m     78\u001b[0m \u001b[39m# Seeds are only used once\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset_seeds()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:106\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys:\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs\n\u001b[1;32m    107\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs[key]\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,96,96,3) into shape (96,96,3)"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path = os.path.join(\"Training\", \"Saved Models\", \"PPO_Car_Racing_multi_input_500k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(ppo_path, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Logs/PPO_16'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `tensorboard --logdir=str(training_log_path)'\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=training_log_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MultiCarRacing-v0\", num_agents=1, direction='CCW',\n",
    "        use_random_direction=True, backwards_flag=True, h_ratio=0.25,\n",
    "        use_ego_color=False)\n",
    "#env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "/home/developer/.local/share/virtualenvs/workspace-LQiMqFvY/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:208: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1013..1278 -> 265-tiles track\n",
      "Track generation: 1027..1288 -> 261-tiles track\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.515151523053646, 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1196..1499 -> 303-tiles track\n",
      "Episode: 1 Score: [-33.77483444]\n",
      "Average time per step: 0.05716s\n"
     ]
    }
   ],
   "source": [
    "# Alternative way of testing the model\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    avg_time_per_step = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action) \n",
    "        score += reward if reward != -100 else 0\n",
    "\n",
    "        # Time per step\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        avg_time_per_step.append(delta)\n",
    "        print(f\"Time per step: {delta:.5f}s\", end=\"\\r\")\n",
    "\n",
    "    print(f\"Episode: {episode} Score: {score}\")\n",
    "    print(f\"Average time per step: {np.mean(avg_time_per_step):.5f}s\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CarRacing-v2\"\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6228343 , 0.10581603, 0.9364147 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.load(\"image.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f44eb2f4eb0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
